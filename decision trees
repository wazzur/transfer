Gerome (B+)

Did not implement extra features

used dictionaries to simulate tree.

iris 15/85, manual, rest automatic
used around 10/90 train/test split on rest.

iris 100/76.1
tictactoe 100/86.5
cars 100/91.1




Maddee Martin ()

did not implement any extra features

manually separated data into 25% test/75% train

built own tree via recursion.

balloons 100/75 
tictactoe 100/100



Alex Cherekdijan (b/b-)

No extra implementation

Built your own tree.

tictactoe 100/77.19
monk-1	100/87.96

split data via manual grabbing (no specific %)



Sarah Johnson (a?)

Used dictionaries to simulate trees.
Implemented random forest (used 75 trees)

tictactoe 100/78.5
/w random forest 0.81.67

balance scale 100/66.4
/w random forest 66.4




Payton Bradsky ()

used anytree api

handled missing data
	- skipping during gain calculation
	- chose most popular choice when missing value was presented.

manually deleted data

Cars - 100/93.6
Cars /w missing - 99.8/95.9
tictactoe - 100/80.2
tictactoe /w missing - 60.2/63.5


Rachel Goldstein (a/a+)

Monks pure & monks with ~5% noise
tictactoe

implemented random forest with sqroot(instances) as number of trees.

accuracy:		with random forest:
tictactoe 100/86.74	100/99.4
monk 100/95.2		99.4/98.85
monk /w noise 100/100	94.2/98.85



Axel Perez (A?) - accuracy went down with pruning

MONK's problem 1 (435 instances)
tic-tac-toe Endgame (~1k)

built own tree (recursive)
implemented pruning 

tested with playtennis sample

data avg of 3 runs

unpruned data:
monk 100/97.4
tictactoe 100/86.66

pruning:
monk 95.65/93.1
tictactoe 90.7/82.7


Colby Harper (A? A+?)


built his own tree

cars (6 attr, 4 outputs, near 2k instances)
iris (4 attr, 150 attributes, 3 outputs)

dealed with continuous values  via buckets ( convert float to int via rounding from pandas)
results showed overfitting due to accuracy difference.

iris 100/86.66
cars 100/60



Chris Yates ()

Used cars and tic-tac-toe
created own tree

No extra implementation

Accuracy on tictactoe:
	train/test: 90/85

accuracy on cars:
	train/test: 86/87



Kelly Ouye ()

used tictactoe and mushroom toxicity

had issue with incorrect labels making accuracy determination difficult.

made slight change to leaf node and code worked just before presentation.

tictactoe testing accuracy: 93%
mushroom toxicity accuracy ~100%

no extra implementation.


Jason ()

No extra implementation

tested on playsports
used datasets: mushrooms and poker hands.

mushrooms:
	binary
	8000+
	22 attributes
	99.7% testing accuracy

pokerhands:
	9 target classes
	25,000+
	10 attributes
	accuracy: 83% on testing data.



Steven Hu ()

used adult-stretch as a 20 sample testing set to write the program.

Used balance(3classes) 625 instances
created own tree.
Wanted to use mushroom toxicity set but ran out of time.
started pruning process but ran out of time.

94% training accuracy on adult-stretch
issue in with determining accuracy as it was handled manually.
Balance dataset never predicted balanced, only left or right.



Kaitlin (B?)

used: Dermatology(6 target classes reduced to is psoriasis or not)  366instances

Breast cancer datasets (simplified 10 attribute values to 2)
classified malignant or not as listed. 683 instances


preprocessed dematology target simplified 0-3 range to 0-1 for each attribute.

No extra implementation.

Dermatology: 98.9/98/100%? (train/vali/test)
breast cancer 89.8/89.2/98.85% 



Abhay (A)

datasets: Loan(binary)614 instances, and wine(11 targets)4900+ quality.
11 attributes each.

tried rounding continuous values up a decimal with negative results, then removed after observation.

implemented random forest(20), chose most popular output.
implemented playtennis with 100% before datasets.

loan accuracy: vali:65, test:60, 
	random forest: 72/63.9

Wine: vali: 55.6, test: 54.4
	random forest: 46.22/48.93








Eli Yale (A?A-?)

Used touchdowns and mountaineering success (kaggle)

touchdowns (3 attr/65,000+)
(likely to not select touchdown)

himalayan peaks(6 attr/ 9610)

Tried to account for missing attributes.
Solved issue via book solution. 
Select the most common value for the attribute.

accuracy for nfl: 97/97
accuracy for himalaya without filling missing attr: 66.7/61.4
			with filling missing attr: 67.1/61.3 
	(random sampling difference, no noticable improvement)



Tristen Islam (a?)

Implemented pruning based on ratio from user

Used tictactoe and SPECT Heart datasets
pruning did not help for tictactoe but helped for heart dataset.

heart accuracy: 96.5% 53% 70.2% (train/vali/test)
	after prune (70.5%/ 60.6%/70.2%)

tictactoe acc: 100/85.5/54.9
	after prune:99/84.9(same test, didnt retry testing)

missing data selects majority


Illyas (b?/b-?)

used letter recognition dataset (16 attr, 26 classes) (20,000)
accuracy: 75/73 (vali/test)


used congressional voting records (numbers flipped in slides)
accuracy: 88/97 (vali/test)

No extra implementations

accuracy on spect heart:



Derek Hu (A/A+)

Office occupancy(1000) and banknote authentication(8000)

Implemented continuous values via optimizing information gains. (Drastic speed decrease tradeoff for good tree building)

time for each dataset: 21sec/23min - thus not feasible on larger sets.

accuracy:
Office: 99.6/97.8
banknote: 99/7/99.5



Tim Shur (A+)
used gain ratio
implemented pruning, removing leaves until it stops being helpful.
Datasets: Nursing school acceptance
	Iris plant species.
also tried chess(17 targets) endgames, but didnt work well.


observed that test accuracy didnt increase much even after pruning on these datasets.

Nursery accuracy: 100%/98%/97% (train/vali/test)
	After prune: 98.7%/98.6%/96.2%

Iris accuracy:	100%/91.6%/93.3%
	After prune: 95%/91.6%/93.3%

chess: 100%/83.3%/65%
	After prune: 89.5/83.6/60.9



Ben Brown (B?)

Used car + Balance scale weight & distance databse.

Implemented random forest.

Balanced scale:

error on random forest 45%/42% (test/train)
error on tree 51%/48%

Cars:
error in random forest: 24%/24%
error in tree 14%/16%



Victor Yu  (Maybe a C?)

used very large datasets (200,000+)
had problems with recursively constructing the tree.

used datasets days it rained in seattle, classifying if it rained or not. Had issues working with first dataset, ~55% accuracy without precipitation.
76/87 accuracy with precipitation column.


Dataset 2 resturants in new york classifying if it is critical to shut down or not.
High data, low accuracy. ~55ish


Connor Carraher (A?A+?)

used titanic and cars

titanic classifying survival
(females had higher survival rate?)

cars classifying acceptable vs unacceptable 
simplified from very acceptable, somewhat acceptable, etc

solved continuous data via bucketing from average (higher or lower)
~81/77 accuracy
~63/65 accuracy


tried missing data with average values.

wanted to use stepghen curry shots, but data was super ambigious.


